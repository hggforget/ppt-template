%!TEX program = xelatex
\documentclass[10pt,mathserif]{beamer}%,aspectratio=169 %画面比例16:9%
\usepackage{ctex}
\usepackage{xeCJK}
\usepackage{listings}
\usepackage{makecell}
\usepackage{setspace}
\usepackage{array}
\usepackage{booktabs} %调整表格线与上下内容的间隔
\usepackage{subcaption}
\usepackage[slide,algoruled]{algorithm2e}
\usepackage{indentfirst}
\usepackage{multirow}
\usepackage{ulem}
\usepackage{microtype}
\usepackage{caption}
\usepackage{fontspec}
\usepackage{xcolor}
\setmainfont{Times New Roman}
\definecolor{ForestGreen}{RGB}{34,139,34}
\definecolor{BrickRed}{RGB}{182,50,28}
\definecolor{OliveGreen}{RGB}{60,128,49}
\definecolor{CadetBlue}{RGB}{116,114,154}
\definecolor{YellowGreen}{RGB}{152,204,112}
\definecolor{DarkerOrange}{RGB}{200,100,0}
\definecolor{myblue1}{RGB}{70,130,180}

\definecolor{codegreen}{rgb}{0,0.5,0}
\definecolor{codered}{rgb}{0.7,0.1,0.1}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{1,1,1}
\newcommand{\newDream}[1]{{\color{codepurple} #1}}
\newcommand{\newpid}[1]{{\color{codegreen} #1}}


\usetheme[
	sidebar, % 会在每页左侧加上导航栏，参考 The AAU Sidebar Beamer Theme 设置
	xdblue, % 不选会默认西电红色调，选上会将主题设置为西电蓝色调
	% english % 选上，会将图表等标题还原回英文标题
 ]{XDUstyle}


\title{无人机智能避障与导航}
\institute[北京航空航天大学\\计算机学院]{北京航空航天大学计算机学院23级硕士研究
生} % 中括号部分为导航栏底所用尽可能精简
\author{黄栋篪}
\date{\today}% 时间可自行设置


\setlength{\footnotesep}{0pt}
\setbeamerfont{footnote mark}{size=\tiny}
\setbeamerfont{footnote text}{size=\tiny}
\setbeamertemplate{footnote}{%
  {\setlength{\hsize}{330pt}%
   \usebeamercolor[fg]{footnote mark}%
   \usebeamerfont*{footnote mark}%
   \hspace*{1.5em}%
   \@thefnmark.~%
   \usebeamercolor[fg]{footnote text}%
   \usebeamerfont*{footnote text}%
   \insertfootnotetext\par}}
\setbeamertemplate{bibliography entry article}{}
\setbeamertemplate{bibliography entry title}{}
\setbeamertemplate{bibliography entry location}{}
\setbeamertemplate{bibliography entry note}{}
\setbeamertemplate{navigation symbols}{}
\def\beamer@newblock{%
  \usebeamercolor[fg]{bibliography entry author}%
  \usebeamerfont{bibliography entry author}%
  \usebeamertemplate{bibliography entry author}%
  \def\newblock{%
    \usebeamercolor[fg]{bibliography entry title}%
    \usebeamerfont{bibliography entry title}%
    \usebeamertemplate{bibliography entry title}%
    \def\newblock{%
      \usebeamercolor[fg]{bibliography entry location}%
      \usebeamerfont{bibliography entry location}%
      \usebeamertemplate{bibliography entry location}%
      \def\newblock{%
        \usebeamercolor[fg]{bibliography entry note}%
        \usebeamerfont{bibliography entry note}%
        \usebeamertemplate{bibliography entry note}}}}}
\makeatother
	
\begin{document}

{\xdbg \frame[plain,noframenumbering]{\titlepage}}%首页标题页

\section{overview}

\begin{frame}[t, fragile]
	\frametitle{overview}
	% \setlength{\parindent}{2em}
	\begin{spacing}{1.5}
		SafeDreamer Project主要分为两个部分:
		\begin{itemize}
			\item RL算法. SafeDreamer 结合了世界模型、拉格朗日约束和CEEM的
			一种基于安全强化学习的智能导航算法
			\item Safety-Gymnasium. Safety-Gymnasium是一个基于MuJoCo引擎的仿真验证环境,
			主要面向SafeRL算法的智能体仿真验证,它支持单、多智能体;基于OpenGL库,支持丰富的视觉输入;
			高度可定制化;支持多种智能体和任务;并且集成一个SafeRL算法库。
		\end{itemize}
	\end{spacing}
\end{frame}

\section{RL算法}
\begin{frame}[t, fragile]
	\frametitle{SafeDreamer Algorithm}
	% \setlength{\parindent}{2em}
	\begin{spacing}{1.3}
		SafeDreamer是World Model, Lagrangian SafeRL methods和CCEM三种方法的结合,最终取得了良好的效果。
		\begin{itemize}
			\item \textbf{World Model:} World Model 是一种用来记忆和建模环境的神经网络模块,
			在SafeDreamer中使用DreamerV3来作为World Model,其神经网络结构为RSSM.
			\item \textbf{Lagrangian SafeRL methods:} Lagrangian SafeRL methods 是一种通过拉格朗日方法来求解强
			化学习中有约束优化问题的一种方法。在SafeDreamer中使用Lag-aug和PID-Lag来提高模型性能。
			\item \textbf{CCEM:} 约束交叉熵方法(CCEM),在每次迭代中，首先从策略分布中取样，选择一组精英样本策略，
			并使用它们来更新策略分布。精英样本：我们使用约束值对样本策略进行排序,然后选择约束性能最好的策略。
		\end{itemize}
	\end{spacing}
\end{frame}

\subsection{Model Components}

\begin{frame}[t, fragile]
	\frametitle{Model Components}
	\begin{spacing}{1.5}
		SafeDreamer由world models和actor-critic models组成.
		\begin{equation*}
			\begin{split}
			\begin{array}{ll}
			\text {Observation encoder:} & z_t \sim E_\phi\left(z_t \mid h_t, o_t\right) \\ 
			\text {Observation decoder:} & \hat{o}_t \sim O_\phi\left(\hat{o}_t \mid s_t\right) \\
			\text {Reward decoder:} & \hat{r}_t \sim R_\phi\left(\hat{r}_t \mid s_t\right)  \\ 
			\text {Cost decoder:} & \hat{c}_t \sim C_\phi\left(\hat{c}_t \mid s_t\right) \\
			%\text {Continue decoder:} & \hat{u}_t \sim U_\phi\left(\hat{u}_t \mid s_t\right) \\
			\text {Sequence model:} & h_t, \hat{z}_t=S_\phi\left(h_{t-1}, z_{t-1}, a_{t-1}\right) \\
			%\text {Dynamics:} & \hat{z}_t \sim p_\phi\left(\hat{z}_t \mid h_t\right)\\
			\text{Actor:} & a_{t} \sim \pi_{\theta}( a_t\mid s_t)\\
			\text{Reward critic:} & \hat{v}_{r_t} \sim V_{\psi_r}(\hat{v}_{r_t} \mid s_t)\\
			\text{Cost critic:} & \hat{v}_{c_t} \sim V_{\psi_c}(\hat{v}_{c_t} \mid s_t)\\
			\end{array}
			\end{split}
		\label{eq:definition}
		\end{equation*}
		
	\end{spacing}
\end{frame}

\subsection{Preliminaries}
\subsubsection{Dreamer}
\begin{frame}[t, fragile]
  \frametitle{World Model}
   \begin{figure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=0.75\linewidth]{SafeDreamer/images/model_wm.pdf}
\caption{World Model Learning}
\end{subfigure}
\begin{subfigure}{.43\textwidth}
\includegraphics[width=0.75\linewidth]{SafeDreamer/images/model_ac.pdf}
\caption{Actor Critic Learning}
\end{subfigure}
\label{fig:model}
\end{figure}
{
\fontsize{8pt}{9.6pt}\selectfont
\begin{itemize}
\item \textbf{包含的模型}：SafeDreamer包括世界模型和行动者-评论者模型。
\item \textbf{处理观察和动作}：在每个时间步骤$t$，世界模型接收一个观察$o_t$和一个动作$a_t$，并将观察压缩为一个离散的表示$z_t$。
\item \textbf{预测任务}：这个离散的表示$z_t$，连同动作，被序列模型用来预测下一个表示$\hat{z}_{t+1}$。
\item \textbf{模型状态的定义}：模型状态$s_t=\{h_t, z_t\}$由一个循环状态$h_t$和表示$z_t$的联接表示。
\item \textbf{解码器的作用}：解码器利用模型状态$s_t$预测观察，奖励和成本。
\item \textbf{行动者-评论者模型的输入}：模型状态$s_t$充当行动者-评论者模型的输入，以预测奖励值$v_{r_t}$，成本值$v_{c_t}$和动作$a_t$。
\end{itemize}
}
\end{frame}

\begin{frame}[fragile]
	\frametitle{Lagrangian-based methods}
	PID 拉格朗日方法是一种广泛应用于约束优化问题的算法。它的学习动态通常会展示振荡和超调，这在保证响应性和牺牲安全性方面是一个挑战。PID 拉格朗日方法利用了 PID 控制器的概念，使用比例、积分和微分这三个成分来平衡和控制学习动态，以优化安全性和效率。
	\begin{algorithm}[H]
		\SetAlgoLined
		\KwIn{Proportional coefficient $K_p$, integral coefficient $K_i$, differential coefficient $K_d$}
		
		Initialize previous integral item: $I^0 \leftarrow 0$\;
		Initialize previous episode cost: $J_C^0 \leftarrow 0$\;
		\While{iteration $k$ continues}{
			Receive cost $J_C^k$\;
			$P \leftarrow J_C^k-d$\;
			$D \leftarrow max(0, J_C^{k}-J_C^{k-1})$\;
			$I^k \leftarrow max(0, I^{k-1}+D)$\;
			$\lambda_p \leftarrow max(0, K_p P+K_i I^k + K_d D)$\;
			Return Lagrangian multiplier $\lambda_p$\;
		}
		\caption{PID Lagrangian}
		\label{alg: pid_lag}
		\end{algorithm}
\end{frame}

\begin{frame}[t, fragile]
	\frametitle{CCEM}
 Constrained Cross-Entropy Method 是一种应用于安全强化学习问题的方法。该方法特点在于，它会显式地跟踪其在满足约束方面的表现。特别是，该方法在有限长度的轨道上定义约束，这是以预期的成本形式表示的。
	\begin{algorithm}[H]
	\SetAlgoLined
	\KwIn{Initial parameters $\theta_0$, initial Lagrange multipliers $\lambda_0$}
	Initialize parameters: $\theta \leftarrow \theta_0$, Lagrange multipliers: $\lambda \leftarrow \lambda_0$\;
	\While{the stop criterion is not met}{
		Generate a set of samples $\mathcal{S}$ under the current model\;
		Compute the utility values $u_s$ and constraint violation $c_s$ for all $s \in \mathcal{S}$\;
		Select a set of "elite" samples $\mathcal{S}_{\text{elite}} = \{s \in \mathcal{S}: u_s \text{ is high and } c_s \text{ is low}\}$\;
		Fit the model parameters $\theta$ with $\mathcal{S}_{\text{elite}}$\;
		Update the Lagrange multipliers $\lambda$ according to $c_s$ and current $\lambda$\;
	}
	\caption{Constrained Cross-Entropy Method}
	\end{algorithm}
	\end{frame}

\subsection{Architecture}
\begin{frame}[t, fragile]
	\frametitle{Architecture}
	\begin{figure}
		\centering
		\includegraphics[width=0.9\linewidth]{./images/brain.pdf}
		\caption{SafeDreamer Architecture}
		\label{Fig:net}
	\end{figure}
	\begin{spacing}{1.5}
		OSPR和BSPR是SafeDreamer中的两种实现方式:
		{
		\fontsize{10}\selectfont
		\begin{itemize}
			\item OSPR:每个决策时间点t都会进行在线规划过程,
			该过程会在世界模型中从当前状态st生成状态-动作轨迹。
			每个轨迹都会通过学习到的奖励和成本模型以及它们的评价者进行评估，
			从中选出最优的安全动作轨迹在环境中执行。
			\item BSPR:在执行者训练期间，
			我们在后台的世界模型中产生长度为T=15的想象的潜在回滚。
			我们从重播缓冲区的观察开始，从执行者中采样动作，
			并从世界模型中获取观察结果。
		\end{itemize}
		}	
	\end{spacing}
\end{frame}

\begin{frame}[t, fragile]
	\frametitle{OSPR}
	\begin{minipage}[t]{0.40\linewidth}
	OSPR在进行轨迹采样的每一步中,会使用Planner生成Action.
	\end{minipage}\hfill
	\begin{minipage}[t]{0.60\linewidth}
	\begin{algorithm}[H]
	\SetAlCapNameFnt{\fontsize{8pt}{9.6pt}\selectfont} 
	\SetAlCapFnt{\fontsize{8pt}{9.6pt}\selectfont}
	\fontsize{8pt}{9.6pt}\selectfont
	\DontPrintSemicolon
	\SetNoFillComment
	\SetAlgoLined
	\textbf{Input:} current model state $s_{t}$, planning horizon $H$,\\
	num sample/policy/safe trajectories $N_{\pi_\mathcal{N}}, N_{\pi_\theta}, N_{s}$,\\
	Lagrangian multiplier $\lambda_p$, cost limit $b$, $\mu^{0}, \sigma^{0}$ for $\mathcal{N}$\\
	\For{$j \gets 1 $ \KwTo J}{
	   Init. empty safe/candidate/elite actions set $A_s, A_c, A_e$\\
	   Sample $N_{\pi_{\mathcal{N}}}+N_{\pi_\theta}$ traj. $\{ s_{i}, a_{i}, s_{i+1}\}_{i=t}^{t+H}$ using $\mathcal{N}(\mu^{j-1}, (\sigma^{j-1})^{2}\mathrm{I})$, $\pi_{\theta}$ within $S_{\phi}$ with  $s_t$ as the initial state\\
	   Select the top-k action trajectories with highest $\Omega$ values among $A_c$ as elite actions $A_e$\\
	   $\mu^j, \sigma^j = \mathrm{MEAN}\left(A_e\right), \mathrm{STD}\left(A_e\right)$ 
	}
	\textbf{Return: } $a \sim \mathcal{N}(\mu^{J}, (\sigma^{J})^{2} \mathrm{I})$
	\caption{Online Safety-Reward Planning.}
	\label{alg:planning}
	\end{algorithm}
	\end{minipage}
	\end{frame}

\begin{frame}[t, fragile]
	\frametitle{BSPR}
	\begin{minipage}[t]{0.20\linewidth}
	BSPR
	\end{minipage}\hfill
	\begin{minipage}[t]{0.80\linewidth}
	\begin{algorithm}[H]
	\SetAlCapNameFnt{\fontsize{6pt}{7.2pt}\selectfont} 
	\SetAlCapFnt{\fontsize{6pt}{7.2pt}\selectfont}
	\fontsize{6pt}{7.2pt}\selectfont
	\DontPrintSemicolon
	\SetNoFillComment
	\SetAlgoLined
	\KwIn{batch length $T$, batch size $B$, episode length $L$, initial Lagrangian multiplier $\lambda_p$}
Initialize the world models parameters $\phi$, actor-critic parameters $\theta, V_{\psi_r}, V_{\psi_c}$ \;
Initialize dataset $\mathcal{D}$ using a random policy\;
\While{\emph{not converged}}{
  Sample $B$ trajectories $\{ o_{t}, a_{t}, o_{t+1}, r_{t+1}, c_{t+1}\}_{t:t+T} \sim \mathcal{D}$ \;
  Update the world models via minimize Equation \eqref{eq:world_model_loss} \;
  Condense $o_{t:t+T}$ into $s_{t: t+T}$ via world models\;
Generate latent rollouts using the actor within the world model with $s_{t: t+T}$ as the initial state\;
  Update the reward and cost critics via minimize Equation \eqref{eq:reward_critic_loss}\;
  \newDream{Update the safe actor and $\lambda_p$ via Equation \eqref{eq:safe_actor_loss} and Equation \eqref{eq:lag}} \tcp*{Only BSRP-Lag}
  $\mathcal{J}_{\mathcal{C}} \leftarrow 0$\;
  \For{$t = 1$ to $L$}{
    Condense $o_t$ into latent state via ${s}_t \sim S_\phi(o_t, s_{t-1}, a_{t-1})$\;
    \newDream{Sample $a_t \sim \pi_a(\cdot | s_t)$}\tcp*{Only BSRP-Lag}
    \newpid{Sample $a_t \sim Planner(s_t, \lambda_p)$}\tcp*{Only OSRP-Lag or OSRP}
    Execute action $a_t$, observe $o_{t + 1},r_{t+1}, c_{t+1}$ returned from the environment\;
    Update dataset $\mathcal{D} \leftarrow \mathcal{D} \cup\left\{o_{t}, a_{t}, o_{t+1}, r_{t+1}, c_{t+1}\right\}$\;
    $\mathcal{J}_{\mathcal{C}} \leftarrow \mathcal{J}_{\mathcal{C}} + c_{t+1}$\;
  }
  \newpid{Use $\mathcal{J}_{\mathcal{C}}$ to update $\lambda_p$ via Algorithm \ref{alg: pid_lag}}\tcp*{Only OSRP-Lag}
}
\caption{~~SafeDreamer. Color \newpid{green is OSRP-Lag or OSRP}, \newDream{purple is BSRP-Lag}}
\label{alg: train_model_ac}
	\end{algorithm}
	\end{minipage}

\end{frame}

\section{Safety-Gymnasium}

\subsection{intro}
\begin{frame}[t, fragile]
\frametitle{Safety-Gymnasium}

\begin{minipage}[t]{\textwidth}
   \textbf{Safety-Gymnasium:} 这是一个专为安全强化学习（SafeRL）构建的仿真环境, 基于 Gymnasium 和 MuJoCo 构建.通过增强现有的 Safety Gym 框架, 支持\textbf{\textcolor{red}{vision-only}}和多智能体任务. 此外, 该项目还集成了 SafePO, 一个单文件风格的算法库, 包含超过 16 种最新的算法。
   
   \vspace{0.3cm}
   
   \textbf{主要组成部分:}
   \vspace{0.2cm}

\begin{enumerate*}{inline=True}
    \item 智能体模型, and-
    \item 安全任务
    \item 安全约束
\end{enumerate*}
\end{minipage}

% \begin{minipage}[t]{\textwidth}
% \begin{columns}
% \begin{column}{0.35\textwidth}
%   \textbf{主要组成部分:}
%   \begin{enumerate}
%     \item 智能体模型
%     \item 安全任务
%     \item 安全约束
%   \end{enumerate}
% \end{column}

% \begin{column}{0.65\textwidth}
% \begin{figure}
%   \centering
%   \includegraphics[width=0.95\textwidth]{SafeDreamer/images/task.pdf}
%   \vspace*{-0.5\baselineskip}
%   \setbeamerfont{caption}{size=\tiny}
%   \caption{Tasks from Safety-Gymnasium.}
%   \label{fig:task}
% \end{figure}
% \end{column}
% \end{columns}
% \begin{figure}
%   \centering
%   \includegraphics[width=0.8\textwidth]{Safety-Gymnasium/assets/main-paper/agent.pdf}
%   \vspace*{-0.5\baselineskip}
%   \setbeamerfont{caption}{size=\tiny}
%   \caption{Tasks from Safety-Gymnasium.}
%   \label{fig:task}
% \end{figure}
% \end{minipage}
\vfill
\begin{minipage}[bp]{\textwidth}

\begin{figure}
  \centering
  \includegraphics[width=0.95\textwidth]{SafeDreamer/images/task.pdf}
  \vspace*{-0.8\baselineskip}
  \caption{Tasks from Safety-Gymnasium.}
  \label{fig:task}
\end{figure}

\end{minipage}

\end{frame}
\subsection{Agents}
\begin{frame}[t,fragile]
    \frametitle{智能体模型}
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.9\linewidth]{Safety-Gymnasium/assets/main-paper/agent.pdf}
  \vspace*{-0.5\baselineskip}
  \setbeamerfont{caption}{size=\tiny}
  \caption{\textbf{Upper: }The Single-Agent Robots. \textbf{Lower:} The Multi-Agent Robots.}
  \label{pic:agent}
\end{figure}
\vspace{-0.5cm}
\begin{itemize}
{
\fontsize{8pt}{9.6pt}\selectfont
    \item \textbf{预设机器人:} Safety-Gymnasium 继承了 Safety Gym 里的三种现有智能体——Point，Car 和 Doggo。
    \item \textbf{优化改进:} 通过精心调整模型参数，成功地缓解了 Point 和 Car 智能体在运行中过度振荡的问题。
    \item \textbf{新增机器人:} 在此基础上，引入了两种额外的机器人 —— racecar 和 ant，以丰富单智能体场景。
    \item \textbf{多智能体机器人:} 从多智能体 MuJoCo 中借鉴了一些配置，解构了原本的单智能体结构，并使多个智能体可以控制不同的身体部分。
}
\end{itemize}
\end{frame}


\subsection{Tasks}
\begin{frame}[t,fragile]
    \frametitle{安全任务}
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.9\linewidth]{Safety-Gymnasium/assets/main-paper/tasks.pdf}
  \caption{Tasks of Gymnasium-based Environments}
  \label{pic:agent}
\end{figure}
\vspace{-0.5cm}
\begin{itemize}
{
\fontsize{8pt}{9.6pt}\selectfont
    \item \textbf{Velocity:} 通过对铰链施加扭矩，使机器人在前向（右）方向上协调腿部运动。
    \item \textbf{Run:} 机器人从随机的初始方向和特定的初始速度开始，努力到达地图的另一边。
    \item \textbf{Circle:} 通过沿着绿色圆移动且不能进入红色区域外部来最大化奖励，因此，它的最优路径遵循 AD 和 BC 的线段。
    \item \textbf{Goal:} 机器人导航至多个目标位置。成功到达一个目标后，其位置在保持整体布局不变的情况下随机重置。
    \item \textbf{Push:} 目标是将一个盒子移动到一系列目标位置。与Goal任务类似，每个成就后会生成新的随机目标位置。
    \item \textbf{Button:} 目标是激活环境中分布的一系列目标按钮。智能体的目标是导航至并接触当前突出显示的按钮，称为目标按钮。
}
\end{itemize}
\end{frame}



\subsection{Constraints}
\begin{frame}[t,fragile]
    \frametitle{安全约束}
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.9\linewidth]{Safety-Gymnasium/assets/main-paper/constraints.pdf}
  \caption{Constraints of Gymnasium-based Environments}
  \label{pic:agent}
\end{figure}
\vspace{-0.5cm}
\begin{spacing}{1.2}
\begin{itemize}
{
\fontsize{8pt}{9.6pt}\selectfont
    \item \textbf{Velocity-Constraint:} 在这些任务中，智能体通过更快地移动来获得更高的奖励，但也必须遵守速度限制以确保安全。
    \item \textbf{Pillars:} 用于在环境中表示大型圆柱形障碍。在一般情况下，接触柱子会产生代价。
    \item \textbf{Hazards:} 用于模拟环境中的危险区域，对智能体进入这些区域造成代价。
    \item \textbf{Sigwalls:} 开发的特殊用于Circle任务。从安全区内部越过围墙到外部会产生代价。
    \item \textbf{Vases:} 代表环境中的静态和易碎物体。触摸或移动这些物体会对智能体产生代价。
    \item \textbf{Gremlins:} 代表环境中可以与智能体互动的移动物体。
}
\end{itemize}
\end{spacing}
\end{frame}



\subsection{Vision-only tasks}
\begin{frame}[t,fragile]
    \frametitle{Vision-only tasks}
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.9\linewidth]{Safety-Gymnasium/assets/main-paper/vision-task.pdf}
  \caption{Vision-only Tasks of Gymnasium-based Environments.}
  \label{pic:agent}
\end{figure}
\vspace{-0.5cm}
\begin{spacing}{1.2}
原始框架的不足：
\begin{itemize}
{
\fontsize{8pt}{9.6pt}\selectfont
   
    \item \textbf{环境逼真度低：}尽管Safety Gym的初始版本提供了基本的视觉输入支持，但其环境的逼真性有待提高。
Safety Gymnasium的改进：
}
\end{itemize}
Safety Gymnasium的改进：
\begin{itemize}
{
\fontsize{8pt}{9.6pt}\selectfont
   
       \item \textbf{提高环境逼真度：}Safety Gymnasium使用MuJoCo模型制定了一个更真实的视觉环境，增强了环境的逼真性。
    \item \textbf{整合了更多的视觉输入：}相较于原始框架，这个改进版的环境能够整合RGB和RGB-D这两种输入，从而提供了更为准确和全面的视觉信息。
}
\end{itemize}
\end{spacing}
\end{frame}

\subsection{SafePO}
\begin{frame}[t,fragile]

    \frametitle{SafePO}
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\linewidth]{Safety-Gymnasium/assets/main-paper/arch.pdf}
  \vspace*{-0.5\baselineskip}
  \caption{The Architecture of \texttt{SafePO}}
  \label{pic:architecture}
\end{figure}
\vspace{-0.5cm}
\begin{itemize}
{
\fontsize{8pt}{9.6pt}\selectfont
    \item \textbf{正确性:} SafePO确保了高度的准确性与可靠性，与现有基准相比具备超越或匹敌的性能。
    \item \textbf{可扩展性:} 新算法可以方便地集成到SafePO中，增强其可扩展性。
    \item \textbf{日志记录和可视化:} SafePO支持TensorBoard和WandB，提供了许多参数和结果的可视化，使训练过程透明化。
    \item \textbf{详细的文档指南:} SafePO配有详尽的文档，从安装指南到高级定制都有涵盖，方便用户参考使用。
}
\end{itemize}

\end{frame}


\end{document} 
